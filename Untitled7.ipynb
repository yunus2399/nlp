{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==3 in c:\\users\\win10\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\win10\\anaconda3\\lib\\site-packages (from transformers==3) (1.19.2)\n",
      "Requirement already satisfied: requests in c:\\users\\win10\\anaconda3\\lib\\site-packages (from transformers==3) (2.24.0)\n",
      "Requirement already satisfied: tokenizers==0.8.0-rc4 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from transformers==3) (0.8.0rc4)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\win10\\anaconda3\\lib\\site-packages (from transformers==3) (0.0.43)\n",
      "Requirement already satisfied: packaging in c:\\users\\win10\\anaconda3\\lib\\site-packages (from transformers==3) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from transformers==3) (2020.10.15)\n",
      "Requirement already satisfied: filelock in c:\\users\\win10\\anaconda3\\lib\\site-packages (from transformers==3) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from transformers==3) (4.50.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\win10\\anaconda3\\lib\\site-packages (from transformers==3) (0.1.94)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from requests->transformers==3) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from requests->transformers==3) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from requests->transformers==3) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from requests->transformers==3) (3.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\win10\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3) (0.17.0)\n",
      "Requirement already satisfied: six in c:\\users\\win10\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\win10\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from packaging->transformers==3) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\win10\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\win10\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\win10\\anaconda3\\lib\\site-packages (from torch) (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many parameters does BERT-large have?\"\n",
    "answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 70 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "how           2,129\n",
      "many          2,116\n",
      "parameters   11,709\n",
      "does          2,515\n",
      "bert         14,324\n",
      "-             1,011\n",
      "large         2,312\n",
      "have          2,031\n",
      "?             1,029\n",
      "\n",
      "[SEP]           102\n",
      "\n",
      "bert         14,324\n",
      "-             1,011\n",
      "large         2,312\n",
      "is            2,003\n",
      "really        2,428\n",
      "big           2,502\n",
      ".             1,012\n",
      ".             1,012\n",
      ".             1,012\n",
      "it            2,009\n",
      "has           2,038\n",
      "24            2,484\n",
      "-             1,011\n",
      "layers        9,014\n",
      "and           1,998\n",
      "an            2,019\n",
      "em            7,861\n",
      "##bed         8,270\n",
      "##ding        4,667\n",
      "size          2,946\n",
      "of            1,997\n",
      "1             1,015\n",
      ",             1,010\n",
      "02            6,185\n",
      "##4           2,549\n",
      ",             1,010\n",
      "for           2,005\n",
      "a             1,037\n",
      "total         2,561\n",
      "of            1,997\n",
      "340          16,029\n",
      "##m           2,213\n",
      "parameters   11,709\n",
      "!               999\n",
      "altogether   10,462\n",
      "it            2,009\n",
      "is            2,003\n",
      "1             1,015\n",
      ".             1,012\n",
      "34            4,090\n",
      "##gb         18,259\n",
      ",             1,010\n",
      "so            2,061\n",
      "expect        5,987\n",
      "it            2,009\n",
      "to            2,000\n",
      "take          2,202\n",
      "a             1,037\n",
      "couple        3,232\n",
      "minutes       2,781\n",
      "to            2,000\n",
      "download      8,816\n",
      "to            2,000\n",
      "your          2,115\n",
      "cola         15,270\n",
      "##b           2,497\n",
      "instance      6,013\n",
      ".             1,012\n",
      "\n",
      "[SEP]           102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
    "# tokenizer's behavior, let's also get the token strings and display them.\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "# For each token and its id...\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    \n",
    "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    \n",
    "    # Print the token string and its ID in two columns.\n",
    "    print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the input_ids for the first instance of the `[SEP]` token.\n",
    "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "# The number of segment A tokens includes the [SEP] token istelf.\n",
    "num_seg_a = sep_index + 1\n",
    "\n",
    "# The remainder are segment B.\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "# Construct the list of 0s and 1s.\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "# There should be a segment_id for every input token.\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our example through the model.\n",
    "start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"340 ##m\"\n"
     ]
    }
   ],
   "source": [
    "# Find the tokens with the highest `start` and `end` scores.\n",
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores)\n",
    "\n",
    "# Combine the tokens in the answer and print it out.\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"340m\"\n"
     ]
    }
   ],
   "source": [
    "# Start with the first token.\n",
    "answer = tokens[answer_start]\n",
    "\n",
    "# Select the remaining answer tokens and join them with whitespace.\n",
    "for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "    # If it's a subword token, then recombine it with the previous token.\n",
    "    if tokens[i][0:2] == '##':\n",
    "        answer += tokens[i][2:]\n",
    "    \n",
    "    # Otherwise, add a space then the token.\n",
    "    else:\n",
    "        answer += ' ' + tokens[i]\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, answer_text):\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "    # Report how long the input sequence is.\n",
    "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # ======== Evaluate ========\n",
    "    # Run our example question through the model.\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
    "\n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Never fear, that's why we are here. You can learn about our time proven process\n",
      "on our process page.Eternity was founded in 2000 by Founder, Mike Lannen. You\n",
      "can learn more on our Team page.The cost of a website can vary depending on\n",
      "various factors, just like the cost of a house may vary. Though our website\n",
      "projects generally start in the $8,500 range for basic business sites and range\n",
      "upward depending on your unique needs.The project starts with a 50% deposit and\n",
      "4 monthly installments of the project balance over the course of development.We\n",
      "work with a broad range of company types small start-ups, large corporations,\n",
      "nonprofits, B2B, B2C and more across many business industries technology, food,\n",
      "apparel, health + beauty, camps, travel, finance, arts, fair trade, and more.Our\n",
      "standard websites take approximately 9-120 days to create. Our E-commerce\n",
      "(online store) websites take approximately 160 days to create. This time will\n",
      "vary from project to project.YES! Everything that we build will be 100% owned by\n",
      "you. Most of our clients stay with us for the life of the website. Our team is\n",
      "just irresistible that way. But if for any reason you decide you want to take\n",
      "your site to another hosting service and get another company to service the\n",
      "site, we will happily assist you in making the transition as effortlessly and\n",
      "efficiently as possible. The website is yours after all, and we want you to take\n",
      "it wherever you go.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "sites_abstract = \"Never fear, that's why we are here. You can learn about our time proven process on our process page.Eternity was founded in 2000 by Founder, Mike Lannen. You can learn more on our Team page.The cost of a website can vary depending on various factors, just like the cost of a house may vary. Though our website projects generally start in the $8,500 range for basic business sites and range upward depending on your unique needs.The project starts with a 50% deposit and 4 monthly installments of the project balance over the course of development.We work with a broad range of company types small start-ups, large corporations, nonprofits, B2B, B2C and more across many business industries technology, food, apparel, health + beauty, camps, travel, finance, arts, fair trade, and more.Our standard websites take approximately 9-120 days to create. Our E-commerce (online store) websites take approximately 160 days to create. This time will vary from project to project.YES! Everything that we build will be 100% owned by you. Most of our clients stay with us for the life of the website. Our team is just irresistible that way. But if for any reason you decide you want to take your site to another hosting service and get another company to service the site, we will happily assist you in making the transition as effortlessly and efficiently as possible. The website is yours after all, and we want you to take it wherever you go.\"\n",
    "print(wrapper.fill(sites_abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query has 316 tokens.\n",
      "\n",
      "Answer: \"$ 8 , 500\"\n"
     ]
    }
   ],
   "source": [
    "question = \"How much does a website cost?\"\n",
    "\n",
    "answer_question(question, bert_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query has 319 tokens.\n",
      "\n",
      "Answer: \"9 - 120 days\"\n"
     ]
    }
   ],
   "source": [
    "question = \"How long does it take to build a website?\"\n",
    "\n",
    "answer_question(question, bert_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
